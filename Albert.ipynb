{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamideh-ghanadian/ChatGPT_for_Suicide_Risk_Assessment_on_Social_Media/blob/main/Albert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxZmmc-4VwYy",
        "outputId": "c0e28f8a-a0ce-4b30-b27c-689597b6e93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cij7UooyXPGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598a8ff5-4b98-4257-cadc-d4c7735a0447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv1zdsHpNLxy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH-FbFj-WgR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "c1e6d854-172d-40bc-9378-5dbbcd79d938"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id label                                          post_body\n",
              "0      152     a                        Need someone to talk to nan\n",
              "1      706     a  _URL_ Please head over to this link.He needs h...\n",
              "2     1288     a  *_PERSON_ do not have to be good. You do not h...\n",
              "3     1500     a  So my friend (m 17) has been diagnosed clinica...\n",
              "4     1690     a  _PERSON_ this in another sub, felt I needed to...\n",
              "5     2071     a  It's a lie.I have been feeling like this for t...\n",
              "6     2103     a  Copy+paste from Op. This happened starting Fri...\n",
              "7     2435     a  Hi everyone, it's my first time posting here. ...\n",
              "8     3344     a  Hay guys.One of my close friends whom I met on...\n",
              "9     4192     a  From r/trees. Wanted to offer my ears to anybo..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fdf9f880-bd8b-4d8c-8023-e168d80508fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "      <th>post_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152</td>\n",
              "      <td>a</td>\n",
              "      <td>Need someone to talk to nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>706</td>\n",
              "      <td>a</td>\n",
              "      <td>_URL_ Please head over to this link.He needs h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1288</td>\n",
              "      <td>a</td>\n",
              "      <td>*_PERSON_ do not have to be good. You do not h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1500</td>\n",
              "      <td>a</td>\n",
              "      <td>So my friend (m 17) has been diagnosed clinica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1690</td>\n",
              "      <td>a</td>\n",
              "      <td>_PERSON_ this in another sub, felt I needed to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2071</td>\n",
              "      <td>a</td>\n",
              "      <td>It's a lie.I have been feeling like this for t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2103</td>\n",
              "      <td>a</td>\n",
              "      <td>Copy+paste from Op. This happened starting Fri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2435</td>\n",
              "      <td>a</td>\n",
              "      <td>Hi everyone, it's my first time posting here. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3344</td>\n",
              "      <td>a</td>\n",
              "      <td>Hay guys.One of my close friends whom I met on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4192</td>\n",
              "      <td>a</td>\n",
              "      <td>From r/trees. Wanted to offer my ears to anybo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdf9f880-bd8b-4d8c-8023-e168d80508fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdf9f880-bd8b-4d8c-8023-e168d80508fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdf9f880-bd8b-4d8c-8023-e168d80508fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# data_path = '/content/drive/MyDrive/PhD/Suicide/Datasets/dataset_1.csv'\n",
        "# data = pd.read_csv('/content/drive/MyDrive/PhD/Suicide/Datasets/dataset_1.csv', encoding='UTF-8')\n",
        "# data = pd.read_csv('/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/500_Reddit_users_posts_labels.csv', encoding='UTF-8')\n",
        "data = pd.read_csv('/content/drive/MyDrive/PhD/Colab_Notebooks/UMD_dataset_processing /data/concat_labeled-data.csv', encoding='UTF-8')\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test = pd.read_csv('/content/drive/MyDrive/PhD/Suicide/Datasets/twitter_unlabeled.csv', encoding='UTF-8')\n",
        "# test.head(10)\n",
        "\n",
        "# clean=test.dropna()\n",
        "# test= clean.reset_index(drop = True)\n",
        "# Labels = list(test['Label'].unique())\n",
        "\n",
        "# test_dataset = []\n",
        "\n",
        "# for index,row in test.iterrows():\n",
        "#   item ={'text':row['text'][2:-2]}\n",
        "#   test_dataset.append(item)\n",
        "# test_dataset\n",
        "# test_data= pd.DataFrame(test_dataset)\n",
        "# test_data.head(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "x8rkfAFlhbjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaBHv_DUQ_3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "1893d74a-4427-4307-ccc1-33267ef9db87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id label                                          post_body\n",
              "0      152     a                        Need someone to talk to nan\n",
              "1      706     a  _URL_ Please head over to this link.He needs h...\n",
              "2     1288     a  *_PERSON_ do not have to be good. You do not h...\n",
              "3     1500     a  So my friend (m 17) has been diagnosed clinica...\n",
              "4     1690     a  _PERSON_ this in another sub, felt I needed to...\n",
              "5     2071     a  It's a lie.I have been feeling like this for t...\n",
              "6     2103     a  Copy+paste from Op. This happened starting Fri...\n",
              "7     2435     a  Hi everyone, it's my first time posting here. ...\n",
              "8     3344     a  Hay guys.One of my close friends whom I met on...\n",
              "9     4192     a  From r/trees. Wanted to offer my ears to anybo..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea6c9423-ac5f-4bd2-870b-9e7879773c05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>label</th>\n",
              "      <th>post_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152</td>\n",
              "      <td>a</td>\n",
              "      <td>Need someone to talk to nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>706</td>\n",
              "      <td>a</td>\n",
              "      <td>_URL_ Please head over to this link.He needs h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1288</td>\n",
              "      <td>a</td>\n",
              "      <td>*_PERSON_ do not have to be good. You do not h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1500</td>\n",
              "      <td>a</td>\n",
              "      <td>So my friend (m 17) has been diagnosed clinica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1690</td>\n",
              "      <td>a</td>\n",
              "      <td>_PERSON_ this in another sub, felt I needed to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2071</td>\n",
              "      <td>a</td>\n",
              "      <td>It's a lie.I have been feeling like this for t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2103</td>\n",
              "      <td>a</td>\n",
              "      <td>Copy+paste from Op. This happened starting Fri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2435</td>\n",
              "      <td>a</td>\n",
              "      <td>Hi everyone, it's my first time posting here. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3344</td>\n",
              "      <td>a</td>\n",
              "      <td>Hay guys.One of my close friends whom I met on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4192</td>\n",
              "      <td>a</td>\n",
              "      <td>From r/trees. Wanted to offer my ears to anybo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea6c9423-ac5f-4bd2-870b-9e7879773c05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea6c9423-ac5f-4bd2-870b-9e7879773c05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea6c9423-ac5f-4bd2-870b-9e7879773c05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "clean=data.dropna()\n",
        "data= clean.reset_index(drop = True)\n",
        "data.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T14y9hTWQ_3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa1fa05-9589-4e80-937f-200aac3ac72b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'b', 'c', 'd']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "labels = list(data['label'].unique())\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8msTbvxQQ_3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66662e65-8fc0-45ee-b1de-e0dd284e7803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0, 'b': 1, 'c': 2, 'd': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# #numeric_labels =[labels.index(label) for label in labels]\n",
        "# label_map =  dict(zip(labels, [1,0]))  #Isar:usually suicide is the positive class\n",
        "# label_map\n",
        "numeric_labels =[labels.index(label) for label in labels]\n",
        "label_map =  dict(zip(labels, numeric_labels))\n",
        "label_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZxqqJf3Q_3f"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "for index,row in data.iterrows():\n",
        "  item ={'label': label_map[row['label']],\n",
        "         'Post':row['post_body']}\n",
        "  dataset.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY5BwU6cQ_3g"
      },
      "outputs": [],
      "source": [
        "dataset_df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4e9elAUEMWbO",
        "outputId": "1bceaa81-e163-4a95-abd1-448df2ea3757"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     label                                               Post\n",
              "0        0  ['Its not a viable option, and youll be leavin...\n",
              "1        1  ['It can be hard to appreciate the notion that...\n",
              "2        2  ['Hi, so last night i was sitting on the ledge...\n",
              "3        3  ['I tried to kill my self once and failed badl...\n",
              "4        1  ['Hi NEM3030. What sorts of things do you enjo...\n",
              "..     ...                                                ...\n",
              "495      0  ['Its not the end, it just feels that way. Or ...\n",
              "496      4  ['It was a skype call, but she ended it and Ve...\n",
              "497      0  ['That sounds really weird.Maybe you were Dist...\n",
              "498      3  ['Dont know there as dumb as it sounds I feel ...\n",
              "499      2  ['&gt;It gets better, trust me.Ive spent long ...\n",
              "\n",
              "[500 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73d7c8f9-64de-443f-8a51-66c481652b57\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Post</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>['Its not a viable option, and youll be leavin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>['It can be hard to appreciate the notion that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>['Hi, so last night i was sitting on the ledge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>['I tried to kill my self once and failed badl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>['Hi NEM3030. What sorts of things do you enjo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0</td>\n",
              "      <td>['Its not the end, it just feels that way. Or ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>4</td>\n",
              "      <td>['It was a skype call, but she ended it and Ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0</td>\n",
              "      <td>['That sounds really weird.Maybe you were Dist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>3</td>\n",
              "      <td>['Dont know there as dumb as it sounds I feel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>2</td>\n",
              "      <td>['&amp;gt;It gets better, trust me.Ive spent long ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73d7c8f9-64de-443f-8a51-66c481652b57')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73d7c8f9-64de-443f-8a51-66c481652b57 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73d7c8f9-64de-443f-8a51-66c481652b57');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##temporary ####remove this\n",
        "# train_data = pd.DataFrame()\n",
        "# train_data['text'] = list(dataset_df['selftext'].values)\n",
        "# train_data['label'] = list(dataset_df['is_suicide'].values)"
      ],
      "metadata": {
        "id": "tsN1VokYjVPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pk--WYDQ_3g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(dataset_df['Post'].values), list(dataset_df['label'].values), test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OZK_IbxMcUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9628cf19-b7ba-437d-b90b-22195af3daae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFaeJSvkQ_3h"
      },
      "outputs": [],
      "source": [
        "# data_path = '/content/drive/MyDrive/Colab_Notebooks/Hamideh/2/shared'\n",
        "\n",
        "train_data = pd.DataFrame()\n",
        "train_data['Post'] = X_train\n",
        "train_data['label'] = y_train\n",
        "# train_data = train_data.to_csv(data_path +'train1.csv') Isar: removed this\n",
        "\n",
        "test_data = pd.DataFrame()\n",
        "test_data['Post'] = list(dataset_df['Post'].values)\n",
        "test_data['label'] = list(dataset_df['label'].values)\n",
        "# test_data = test_data.to_csv(data_path +'test1.csv')  Isar: removed this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OovtQDuGQ_3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec10c083-8eea-42bc-ac46-c80e8e1343b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"albert-base-v2\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"albert-base-v2\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer,AlbertConfig, AlbertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djSLKSIOQ_3h"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTohyU1bd-re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faa4897-5897-4cb9-f1ed-80cecedfcfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/config.json\n",
            "Model config AlbertConfig {\n",
            "  \"_name_or_path\": \"albert-base-v2\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--albert-base-v2/snapshots/51dbd9db43a0c6eba97f74b91ce26fface509e0b/pytorch_model.bin\n",
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=4)  #https://huggingface.co/models\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_2\", num_labels=2)  #https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gwLRkDuJ1VU"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE65dDQFKe2O"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_data['Post'].values.tolist(), truncation=True, padding=True,return_tensors=\"pt\", max_length=512)\n",
        "train_labels =train_data['label'].values.tolist()  #placeholders, not real labels\n",
        "\n",
        "train_dataset = MyDataset(train_encodings, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxhKC1Sfe-Ri",
        "outputId": "ef6aa1c5-8b66-438b-9e10-75a4d823b5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MyDataset at 0x7f02dda7a280>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = tokenizer(test_data['Post'].values.tolist(), truncation=True, padding=True,return_tensors=\"pt\", max_length=512)\n",
        "test_labels =test_data['label'].values.tolist()  #placeholders, not real labels\n",
        "\n",
        "test_dataset = MyDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "lhZqgPjo1Dp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WasCMg5f1PJ2",
        "outputId": "cb6ff22a-3fb5-4792-ec89-c6d4e562134a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MyDataset at 0x7f1da54eec70>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwnm1bx9K7n1"
      },
      "outputs": [],
      "source": [
        "# # test_encodings = tokenizer(test_data['text'].values.tolist(), truncation=True, padding=True,return_tensors=\"pt\", max_length=512)\n",
        "# # test_labels =test_data['class'].values.tolist()  #placeholders, not real labels\n",
        "\n",
        "# # test_dataset = MyDataset(test_encodings, test_labels)\n",
        "\n",
        "\n",
        "# #For labeling\n",
        "# test_encodings = tokenizer(test_data['text'].values.tolist(), truncation=True, padding=True,return_tensors=\"pt\", max_length=512)\n",
        "# test_labels =test_data['text'].values.tolist()  #placeholders, not real labels\n",
        "\n",
        "# test_encodings\n",
        "# # test = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),torch.tensor(test_encodings['token_type_ids']),torch.tensor(test_encodings['attention_mask']))\n",
        "\n",
        "# # class MyDataset1(torch.utils.data.Dataset):\n",
        "# #     def __init__(self, encodings):\n",
        "# #         self.encodings = encodings\n",
        "        \n",
        "\n",
        "# #     def __getitem__(self, idx):\n",
        "# #         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "# #         return item\n",
        "\n",
        "# #     def __len__(self):\n",
        "# #             return len(self.encodings)\n",
        "\n",
        "# test = MyDataset(test_encodings,test_labels)  \n",
        "# test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5o714OT8p3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97962595-ebc9-4773-c4ef-6da1e492819e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/dist-packages (0.0.post1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "!pip install sklearn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "# def compute_metrics(eval_pred):\n",
        "#     metrics = [\"accuracy\", \"recall\", \"precision\", \"f1\"] #List of metrics to return\n",
        "#     metric={}\n",
        "#     for met in metrics:\n",
        "#        metric[met] = load_metric(met)\n",
        "#     logits, labels = eval_pred\n",
        "#     predictions = np.argmax(logits, axis=-1)\n",
        "#     metric_res={}\n",
        "#     for met in metrics:\n",
        "#        metric_res[met]=metric[met].compute(predictions=predictions, references=labels)[met]\n",
        "#     return metric_res\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    metrics = ['recall', 'precision', 'f1']\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compute accuracy\n",
        "    acc = metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Compute precision, recall, and F1 score for each class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "\n",
        "    # Create a dictionary to store the results\n",
        "    metric_res = {'accuracy': acc['accuracy'], 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "    return metric_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkAL2W_CeC1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d332e5e-38d1-41b4-f7b1-8c25f21a41b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy = \"epoch\", #To calculate metrics per epoch\n",
        "    logging_strategy=\"epoch\", #Extra: to log training data stats for loss \n",
        ")\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_info()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dh7oc_6xTlTq",
        "outputId": "356a4c3b-4b5c-483a-a048-0773ab627a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 692\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 865\n",
            "  Number of trainable parameters = 11686660\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='865' max='865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [865/865 02:11, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.044100</td>\n",
              "      <td>0.044703</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.987044</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.986964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.044000</td>\n",
              "      <td>0.044703</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.987044</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.986964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>0.044703</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.987044</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.986964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.044900</td>\n",
              "      <td>0.044703</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.987044</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.986964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.045700</td>\n",
              "      <td>0.044703</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.987044</td>\n",
              "      <td>0.986994</td>\n",
              "      <td>0.986964</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 692\n",
            "  Batch size = 4\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 692\n",
            "  Batch size = 4\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Saving model checkpoint to /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels/out/checkpoint-500/special_tokens_map.json\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 692\n",
            "  Batch size = 4\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 692\n",
            "  Batch size = 4\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 692\n",
            "  Batch size = 4\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=865, training_loss=0.044832216935350715, metrics={'train_runtime': 131.3449, 'train_samples_per_second': 26.343, 'train_steps_per_second': 6.586, 'total_flos': 82703779676160.0, 'train_loss': 0.044832216935350715, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "trainer.train()\n",
        "# trainer.train(resume_from_checkpoint = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labels'\n",
        "trainer.save_model(model_dir + 'trained_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm2Li6Xd1vmy",
        "outputId": "80028485-6ea6-4fb1-90cf-728aa633973a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labelstrained_model\n",
            "Configuration saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labelstrained_model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labelstrained_model/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labelstrained_model/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/500_Reddit_users_posts_labelstrained_model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "zwLiCCAd1vjt",
        "outputId": "cb19d65c-c156-4377-ef35-42452588b69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 865\n",
            "  Batch size = 4\n",
            "<ipython-input-41-26dfdb2d573e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='217' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [217/217 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5090343952178955,\n",
              " 'eval_accuracy': 0.8890173410404625,\n",
              " 'eval_precision': 0.8939959799893552,\n",
              " 'eval_recall': 0.8890173410404625,\n",
              " 'eval_f1': 0.8904208410262753,\n",
              " 'eval_runtime': 9.0536,\n",
              " 'eval_samples_per_second': 95.542,\n",
              " 'eval_steps_per_second': 23.968,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QsL5Nyh01yMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##predicting labels of twitter dataset"
      ],
      "metadata": {
        "id": "bnj6jzLW3etO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=trainer.predict(test)\n",
        "predictions\n",
        "predicted_label_indices = predictions.predictions.argmax(axis=1)\n",
        "predicted_labels = [trainer.model.config.id2label[i] for i in predicted_label_indices]\n"
      ],
      "metadata": {
        "id": "mfI9fj769HQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "texts = [sample['input_ids'] for sample in test]\n",
        "\n",
        "# Write the predicted labels and texts to a CSV file\n",
        "with open('predictions.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['text', 'predicted_label'])\n",
        "    for text, predicted_label in zip(texts, predicted_labels):\n",
        "        writer.writerow([text, predicted_label])"
      ],
      "metadata": {
        "id": "fvaLsmjh_UTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N3YoIMwgpMJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WS12YRFzgpMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nm0GYkVngpMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCK9_u3bgo4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##second evaluation on smaller dataset"
      ],
      "metadata": {
        "id": "X9OXm4O-kkM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "OCyBodWukfPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFDy64aVT1XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702e6461-e793-48dc-f7ac-0674982f89f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_3\n",
            "Configuration saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_3/config.json\n",
            "Model weights saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_3/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_3/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Alberttrained_model_3/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "model_dir = '/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Albert'\n",
        "trainer.save_model(model_dir + 'trained_model_3') #new messy data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ht2q4mwnQ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First evaluation on bigger dataset"
      ],
      "metadata": {
        "id": "MccQEEgikr2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "pNds7zcKnoAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJX71QTndKIh"
      },
      "outputs": [],
      "source": [
        "predictions=trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbIUdUvyUSan"
      },
      "outputs": [],
      "source": [
        "test_data['predicted'] = predictions.label_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[test_data['Label']==1][test_data['predicted']==0]"
      ],
      "metadata": {
        "id": "EWH4kXd2mndr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[test_data['Label']==0][test_data['predicted']==1]"
      ],
      "metadata": {
        "id": "lntyQ5XGmpEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zSbnNWdSMhB"
      },
      "source": [
        "##**previous code**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeXEcNvQWqzj"
      },
      "outputs": [],
      "source": [
        "# data.head(n = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzJTcGxhQWP"
      },
      "outputs": [],
      "source": [
        "# clean=data.dropna()\n",
        "# data= clean.reset_index(drop = True)\n",
        "# data.head(10)\n",
        "# # texts_encodings = tokenizer(texts, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH4Hk7qXYRAw"
      },
      "outputs": [],
      "source": [
        "# labels = list(data['class'].unique())\n",
        "# labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KocbmvyGYKsV"
      },
      "outputs": [],
      "source": [
        "# numeric_labels =[labels.index(label) for label in labels]\n",
        "# label_map =  dict(zip(labels, numeric_labels))\n",
        "# label_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y-BccYYXj5-"
      },
      "outputs": [],
      "source": [
        "# dataset = []\n",
        "# for index,row in data.iterrows():\n",
        "#   item ={'class': label_map[row['class']],\n",
        "#          'text':row['text']}\n",
        "#   dataset.append(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yEXlE_bZ__Y"
      },
      "outputs": [],
      "source": [
        "# dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNGgFgf3dOsD"
      },
      "outputs": [],
      "source": [
        "# dataset_df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6EJQg-ydWEU"
      },
      "outputs": [],
      "source": [
        "# dataset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWToBIhCeha5"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(list(dataset_df['text'].values), list(dataset_df['class'].values), test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzf7zhlSjhJF"
      },
      "outputs": [],
      "source": [
        "# data_path = '/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Albert/'\n",
        "\n",
        "# train_data = pd.DataFrame()\n",
        "# train_data['text'] = X_train\n",
        "# train_data['label'] = y_train\n",
        "# train_data = train_data.to_csv(data_path +'train.csv')\n",
        "\n",
        "# test_data = pd.DataFrame()\n",
        "# test_data['text'] = X_test\n",
        "# test_data['label'] = y_test\n",
        "# test_data = test_data.to_csv(data_path +'test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhrg5a4pkcN9"
      },
      "outputs": [],
      "source": [
        "# data_path = '/content/drive/MyDrive/PhD/Colab_Notebooks/Model/data_and_results/Albert/'\n",
        "\n",
        "# dataset = load_dataset('csv', data_files={'train': data_path +'train.csv', 'test': data_path +'test.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whnbz9cOkwZS"
      },
      "outputs": [],
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou6tFvEuaD7B"
      },
      "outputs": [],
      "source": [
        "# # !pip install sentencepiece\n",
        "# from transformers import AutoTokenizer,AlbertConfig, AlbertModel\n",
        "# import torch\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9ilAC5JbDHL"
      },
      "outputs": [],
      "source": [
        "# def preprocess_function(examples):\n",
        "#     return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7xwzQm_k3in"
      },
      "outputs": [],
      "source": [
        "# tokenized_data = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YMalMoFdzI4"
      },
      "outputs": [],
      "source": [
        "# from transformers import DataCollatorWithPadding\n",
        "\n",
        "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDTWj2ktpVvB"
      },
      "outputs": [],
      "source": [
        "# from transformers import AlbertConfig, AlbertModel\n",
        "\n",
        "# # Initializing an ALBERT-xxlarge style configuration\n",
        "# albert_xxlarge_configuration = AlbertConfig()\n",
        "\n",
        "# # Initializing an ALBERT-base style configuration\n",
        "# albert_base_configuration = AlbertConfig(\n",
        "#     hidden_size=768,\n",
        "#     num_attention_heads=12,\n",
        "#     intermediate_size=3072,\n",
        "# )\n",
        "\n",
        "# # Initializing a model (with random weights) from the ALBERT-base style configuration\n",
        "# model = AlbertModel(albert_xxlarge_configuration)\n",
        "\n",
        "# # Accessing the model configuration\n",
        "# configuration = model.config"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "2zSbnNWdSMhB"
      ],
      "gpuClass": "premium",
      "mount_file_id": "1sNxpGDZ4Upr64zQybsalHy7mA3SsIKIZ",
      "authorship_tag": "ABX9TyPUt/hMemJvdcmoKKH+nx7Z",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}